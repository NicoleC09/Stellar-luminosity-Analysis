{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df225d7c",
   "metadata": {},
   "source": [
    "# Stellar luminosity:\n",
    "## Polynomial Model for Regression\n",
    "\n",
    "In this notebook, Polynomial regression will be implemented to model the mass-luminosity relationship of stars. \n",
    "\n",
    "### Objective\n",
    "\n",
    "Capture nonlinear and interaction effects using polynomial feature engineering and an explicit bias term:\n",
    "\n",
    "L_hat = X @ w + b.\n",
    "\n",
    "### Dataset and Notation\n",
    "\n",
    "We will use the following notation for the notebook\n",
    "\n",
    "- **M:** stellar mass (in units of solar mass, M⊙)\n",
    "- **L:** stellar luminosity (in units of solar luminosity, L⊙)\n",
    "- **T:** effective stellar temperature (Kelvin, K)\n",
    "\n",
    "And the dataset for this first part is:\n",
    "\n",
    "M = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "L = [0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0]\n",
    "T = [3800, 4400, 5800, 6400, 6900, 7400, 7900, 8300, 8800, 9200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set variables in arrays (one feature)\n",
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "T = np.array([3800, 4400, 5800, 6400, 6900, 7400, 7900, 8300, 8800, 9200])\n",
    "\n",
    "N = len(L)  # number of data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aad17",
   "metadata": {},
   "source": [
    "#### 1. Dataset visualization: M vs L, encoding T\n",
    "\n",
    "We first visualize the relationship between stellar mass (M) and luminosity (L).\n",
    "The effective temperature (T) is encoded using color to observe how temperature\n",
    "correlates with luminosity at different masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "scatter = plt.scatter(M, L, c=T, s=80)\n",
    "plt.xlabel(\"Stellar Mass M (M☉)\")\n",
    "plt.ylabel(\"Luminosity L (L☉)\")\n",
    "plt.title(\"Luminosity vs Mass (color = Temperature)\")\n",
    "plt.colorbar(scatter, label=\"Temperature (K)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c4829",
   "metadata": {},
   "source": [
    "Luminosity increases non-linearly with mass. Higher temperatures generally\n",
    "correspond to higher luminosities, suggesting that temperature should be included\n",
    "as an explanatory feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7218c9",
   "metadata": {},
   "source": [
    "#### 2. Feature engineering: Design matrix X\n",
    "\n",
    "Construct the design matrix using the following features (do not include a constant column of ones):\n",
    "\n",
    "$$X = [ M, T, M^2, M*T ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a081c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(M, T, model=\"M3\"):\n",
    "    if model == \"M1\":\n",
    "        return np.column_stack([M, T])\n",
    "    elif model == \"M2\":\n",
    "        return np.column_stack([M, T, M**2])\n",
    "    elif model == \"M3\":\n",
    "        return np.column_stack([M, T, M**2, M*T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec893db",
   "metadata": {},
   "source": [
    "#### 3. Loss function and gradients (vectorized)\n",
    "\n",
    "##### Model\n",
    "\n",
    "In this notebook, a **polynomial regression model** is used.\n",
    "\n",
    "\n",
    "$$\\hat{L} = Xw + b$$\n",
    "\n",
    "\n",
    "#### MSE\n",
    "\n",
    "And in this notebook, also the **Mean Squared Error** is used.\n",
    "\n",
    "$$J(w,b) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{L}_i - L_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ef0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "\n",
    "def mse(X, L, w, b):\n",
    "    error = predict(X, w, b) - L\n",
    "    return np.mean(error**2)\n",
    "\n",
    "\n",
    "def gradients(X, L, w, b):\n",
    "    error = predict(X, w, b) - L\n",
    "    dw = (2 / N) * X.T @ error\n",
    "    db = (2 / N) * np.sum(error)\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad95da",
   "metadata": {},
   "source": [
    "#### 4. Gradient Descent + Convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, L, lr=0.01, iterations=5000):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0.0\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        dw, db = gradients(X, L, w, b)\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        loss_history.append(mse(X, L, w, b))\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_norm = (M - np.mean(M)) / np.std(M)\n",
    "T_norm = (T - np.mean(T)) / np.std(T)\n",
    "L_norm = (L - np.mean(L)) / np.std(L)\n",
    "\n",
    "\n",
    "#Convergence plot (example with M3)\n",
    "X3 = build_X(M_norm, T_norm, \"M3\")\n",
    "w3, b3, loss3 = gradient_descent(X3, L_norm)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss3)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Convergence of Gradient Descent (M3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cdbe7",
   "metadata": {},
   "source": [
    "The loss decreases smoothly, indicating stable convergence of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ceebc",
   "metadata": {},
   "source": [
    "#### 5. Feature selection experiment (MANDATORY)\n",
    "\n",
    "Models:\n",
    "- **M1:** [M, T]\n",
    "- **M2:** [M, T, M²]\n",
    "- **M3:** [M, T, M², M·T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"M1\", \"M2\", \"M3\"]\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    X = build_X(M_norm, T_norm, model)\n",
    "    w, b, loss = gradient_descent(X, L_norm)\n",
    "    results[model] = (X, w, b, loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a201f6",
   "metadata": {},
   "source": [
    "##### Predicted vs Actual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124547c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, (X, w, b, final_loss) in results.items():\n",
    "    L_pred = predict(X, w, b)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(L_norm, L_pred)\n",
    "    plt.plot([0, 5], [0, 5], \"--\")\n",
    "    plt.xlabel(\"Actual Luminosity\")\n",
    "    plt.ylabel(\"Predicted Luminosity\")\n",
    "    plt.title(f\"{model} – Predicted vs Actual (Loss={final_loss:.3f})\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{model}:\")\n",
    "    print(\"w =\", w)\n",
    "    print(\"b =\", b)\n",
    "    print(\"Final loss =\", final_loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb142916",
   "metadata": {},
   "source": [
    "Model M3 achieves the lowest loss, indicating that both non-linear and interaction\n",
    "terms improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845858b0",
   "metadata": {},
   "source": [
    "#### 6. Cost vs interaction coefficient w_MT (MANDATORY)\n",
    "\n",
    "We vary the interaction coefficient $w_{MT}$ while keeping all other parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fixed = w3.copy()\n",
    "w_mt_values = np.linspace(w3[-1] - 1e-6, w3[-1] + 1e-6, 100)\n",
    "costs = []\n",
    "\n",
    "for val in w_mt_values:\n",
    "    w_test = w_fixed.copy()\n",
    "    w_test[-1] = val\n",
    "    costs.append(mse(X3, L, w_test, b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(w_mt_values, costs)\n",
    "plt.xlabel(\"Interaction coefficient w_MT\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Cost vs Interaction Term\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d7e69",
   "metadata": {},
   "source": [
    "The cost curve exhibits a clear minimum, indicating that the interaction term\n",
    "between mass and temperature plays a significant role in explaining luminosity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2518519",
   "metadata": {},
   "source": [
    "#### 7. Inference demo (MANDATORY)\n",
    "\n",
    "New star:\n",
    "- M = 1.3\n",
    "- T = 6600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a08eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_new = np.array([1.3])\n",
    "T_new = np.array([6600])\n",
    "\n",
    "X_new = build_X(M_new, T_new, \"M3\")\n",
    "L_new_pred = predict(X_new, w3, b3)\n",
    "\n",
    "L_new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5e410",
   "metadata": {},
   "source": [
    "The predicted luminosity lies between nearby observed values, making it physically\n",
    "reasonable given the star's mass and temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448274c",
   "metadata": {},
   "source": [
    "### Final\n",
    "\n",
    "Including polynomial and interaction features significantly improves model accuracy.\n",
    "This demonstrates that stellar luminosity depends on both non-linear mass effects\n",
    "and mass–temperature coupling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
