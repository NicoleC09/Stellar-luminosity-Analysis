{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df225d7c",
   "metadata": {},
   "source": [
    "# Stellar luminosity:\n",
    "## Linear Model for Regression\n",
    "\n",
    "In this notebook, linear regression will be implemented to model the mass-luminosity relationship of stars. \n",
    "\n",
    "### Objective\n",
    "\n",
    "Model stellar luminosity as a function of stellar mass using linear regression with an explicit bias term: L_hat = w * M + b.\n",
    "\n",
    "### Dataset and Notation\n",
    "\n",
    "We will use the following notation for the notebook\n",
    "\n",
    "- **M:** stellar mass (in units of solar mass, M⊙)\n",
    "- **L:** stellar luminosity (in units of solar luminosity, L⊙)\n",
    "\n",
    "And the dataset for this first part is:\n",
    "\n",
    "M = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "L = [0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set variables in arrays (one feature)\n",
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "\n",
    "N = len(L)  # number of data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aad17",
   "metadata": {},
   "source": [
    "#### 1. Dataset visualization: M vs L\n",
    "\n",
    "The graph allows us to see if the relationship between L and M is approximately linear and if a linear model is plausibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(L, M)\n",
    "plt.xlabel(\"L (Luminosity)\")\n",
    "plt.ylabel(\"M (Mass)\")\n",
    "plt.title(\"Mass vs Luminosity of Stars\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec893db",
   "metadata": {},
   "source": [
    "#### 2. Model and loss (MSE)\n",
    "\n",
    "##### Model\n",
    "\n",
    "In this notebook, a **simple linear regression model** is used, which assumes a linear relationship between luminosity \\(L\\) and mass \\(M\\):\n",
    "\n",
    "\n",
    "$$\\hat{M} = wL + b$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "- \\(w\\) is the **slope**, which indicates how much the mass changes when luminosity increases.\n",
    "- \\(b\\) is the **intercept**, which represents the estimated mass when \\(L = 0\\).\n",
    "\n",
    "\n",
    "This model serves as a simple first approximation to describe the relationship between these two physical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b85b05",
   "metadata": {},
   "source": [
    "##### Loss Function (Mean Squared Error)\n",
    "\n",
    "To evaluate how well the model fits the data, the **Mean Squared Error (MSE)** is used:\n",
    "\n",
    "\n",
    "$$J(w,b) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{M}_i - M_i)^2$$\n",
    "\n",
    "\n",
    "This function measures the average of the squared differences between the predicted values and the true values.\n",
    "\n",
    "\n",
    "**Reasons for using MSE:**\n",
    "- It strongly penalizes large errors.\n",
    "- It is always non-negative.\n",
    "- It is differentiable, which allows the use of **gradient descent**.\n",
    "- It is a standard loss function for regression problems.\n",
    "\n",
    "\n",
    "The goal of the training process is to find the values of \\(w\\) and \\(b\\) that **minimize this loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(L, w, b):\n",
    "    return w * L + b\n",
    "\n",
    "def mse(L, M, w, b):\n",
    "    M_pred = predict(L, w, b)\n",
    "    return np.mean((M_pred - M) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad95da",
   "metadata": {},
   "source": [
    "#### 3. Cost surface (mandatory)\n",
    "\n",
    "To better understand the behavior of the loss function, we evaluate the cost\n",
    "\\( J(w, b) \\) over a grid of possible values of the parameters \\( w \\) (slope) and\n",
    "\\( b \\) (intercept).\n",
    "\n",
    "\n",
    "For each pair \\((w, b)\\), the Mean Squared Error (MSE) is computed using the training\n",
    "data. This allows us to visualize how the error changes depending on the model\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vals = np.linspace(-10, 10, 100)\n",
    "b_vals = np.linspace(-10, 10, 100)\n",
    "\n",
    "W, B = np.meshgrid(w_vals, b_vals)\n",
    "J = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(w_vals)):\n",
    "    for j in range(len(b_vals)):\n",
    "        J[j, i] = mse(L, M, W[j, i], B[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ce4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.contour(W, B, J, levels=50)\n",
    "plt.xlabel(\"w\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.title(\"Cost Surface (Contours)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cdbe7",
   "metadata": {},
   "source": [
    "The minimum of this surface represents the pair (w, b) that minimizes the mean square error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ceebc",
   "metadata": {},
   "source": [
    "#### 4. Gradients: dJ/dw and dJ/db.\n",
    "\n",
    "The gradients of the MSE with respect to \\( w \\) and \\( b \\) indicate how to update the\n",
    "parameters in order to minimize the cost function using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(L, M, w, b):\n",
    "    M_pred = predict(L, w, b)\n",
    "    error = M_pred - M\n",
    "    \n",
    "    dw = (2 / N) * np.sum(error * L)\n",
    "    db = (2 / N) * np.sum(error)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845858b0",
   "metadata": {},
   "source": [
    "#### 5. Gradient descent (non-vectorized)\n",
    "\n",
    "- For each iteration, the algorithm loops over all samples.\n",
    "- The prediction error is computed one data point at a time.\n",
    "- Gradients for `w` and `b` are accumulated using an explicit loop.\n",
    "- Parameters are updated in the direction that reduces the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_loop(L, M, lr, iterations):\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        dw, db = 0.0, 0.0\n",
    "        \n",
    "        for i in range(N):\n",
    "            error = (w * L[i] + b) - M[i]\n",
    "            dw += error * L[i]\n",
    "            db += error\n",
    "        \n",
    "        dw *= (2 / N)\n",
    "        db *= (2 / N)\n",
    "        \n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        loss_history.append(mse(L, M, w, b))\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2518519",
   "metadata": {},
   "source": [
    "#### 6. Gradient descent (vectorized)\n",
    "\n",
    "In the vectorized implementation, gradients are computed using NumPy operations instead of explicit loops over the samples.\n",
    "This approach is mathematically equivalent to the non-vectorized version but is significantly more efficient and concise.\n",
    "\n",
    "\n",
    "Vectorization leverages optimized linear algebra routines, making it the standard approach in practical machine learning implementations.\n",
    "It also improves code readability and reduces the risk of implementation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a08eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_vectorized(L, M, lr, iterations):\n",
    "    w, b = 0.0, 0.0\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        dw, db = gradients(L, M, w, b)\n",
    "        \n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        loss_history.append(mse(L, M, w, b))\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448274c",
   "metadata": {},
   "source": [
    "#### 7. Convergence (mandatory)\n",
    "\n",
    "The loss decreases monotonically with the number of iterations, indicating that gradient descent is converging correctly.\n",
    "\n",
    "\n",
    "For an appropriate learning rate, the curve is smooth and stable, showing no oscillations or divergence.\n",
    "\n",
    "\n",
    "This behavior suggests that the algorithm is efficiently moving toward the minimum of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, loss = gradient_descent_vectorized(L, M, lr=0.01, iterations=100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Convergencia del Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144cc29",
   "metadata": {},
   "source": [
    "The curve shows how the error progressively decreases, indicating convergence of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ac2a5",
   "metadata": {},
   "source": [
    "#### 8. Experiments with different learning rates\n",
    "\n",
    "We trained the linear regression model using three different learning rates to analyze their effect on convergence.\n",
    "\n",
    "\n",
    "- **Low learning rate (e.g., 0.001):**\n",
    "Converges slowly but is very stable. Requires more iterations to reach a low loss.\n",
    "\n",
    "\n",
    "- **Medium learning rate (e.g., 0.01):**\n",
    "Provides a good balance between convergence speed and stability. This learning rate achieved the lowest final loss in a reasonable number of iterations.\n",
    "\n",
    "\n",
    "- **High learning rate (e.g., 0.1):**\n",
    "Converges faster initially but may show oscillations or instability near the minimum.\n",
    "\n",
    "\n",
    "Overall, the experiment shows that the choice of learning rate strongly affects both convergence speed and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.00001]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    w, b, loss = gradient_descent_vectorized(L, M, lr, 200)\n",
    "    results[lr] = (w, b, loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, (w, b, final_loss) in results.items():\n",
    "    print(f\"LR={lr} → w={w:.4f}, b={b:.4f}, loss={final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640681c7",
   "metadata": {},
   "source": [
    "Large learning rates converge faster but can be unstable; small values are more stable but slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74a72a",
   "metadata": {},
   "source": [
    "#### 9. Final fit plot\n",
    "\n",
    "The regression line captures the overall trend of the data, indicating that a linear relationship between L and M is a reasonable first approximation.\n",
    "\n",
    "\n",
    "However, systematic errors are visible in certain regions, where the model tends to overestimate or underestimate the true values. This suggests that the underlying relationship may not be perfectly linear and that more complex models could provide a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_mean, L_std = np.mean(L), np.std(L)\n",
    "M_mean, M_std = np.mean(M), np.std(M)\n",
    "\n",
    "L_norm = (L - L_mean) / L_std\n",
    "M_norm = (M - M_mean) / M_std\n",
    "\n",
    "best_lr = 0.001\n",
    "w, b, _ = gradient_descent_vectorized(L, M, best_lr, 2000)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(L, M, label=\"Datos reales\")\n",
    "plt.plot(L, predict(L, w, b), label=\"Modelo lineal\")\n",
    "plt.xlabel(\"L\")\n",
    "plt.ylabel(\"M\")\n",
    "plt.legend()\n",
    "plt.title(\"Ajuste final del modelo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5af1ed",
   "metadata": {},
   "source": [
    "#### 10. Conceptual questions\n",
    "\n",
    "1. **Astrophysical meaning of w:**\n",
    "    It represents the rate of change of mass with respect to luminosity.\n",
    "\n",
    "2. **Limitations of the linear model:**\n",
    "    Many astrophysical processes are nonlinear and cannot be accurately described by a simple linear relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
